{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQb0EiAQSjM"
      },
      "source": [
        "# **Implementation of PPO in Mujoco environement.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqlwFYxXQjjI"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiEl1kxtiwpl"
      },
      "outputs": [],
      "source": [
        "# Installation in colab\n",
        "!pip install mujoco\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install piglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SKjlwtyi8g6"
      },
      "outputs": [],
      "source": [
        "#Drive connexion\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Zp9t0VixNw",
        "outputId": "f68cfde5-1a4c-40e4-f812-501dccb54584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/testppo/PPO ant/PPO-pytorch-Mujoco-master\n"
          ]
        }
      ],
      "source": [
        "# Directory of your folder\n",
        "%cd /your_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK9G2LZuixP-"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries and fundamental coded class.\n",
        "import gym\n",
        "import sys\n",
        "import torch\n",
        "import mujoco\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "from parameters import *\n",
        "from PPO import Ppo,Normalize\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvb6OFnGPmPR"
      },
      "source": [
        "## Run - MAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg1irverTlpi"
      },
      "source": [
        "These are the parameters of *parameters.py* file, they can be adapted in this script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_hPlUlUQGdJ"
      },
      "outputs": [],
      "source": [
        "#learning rate backward propagation NN action (Paper recommendation)\n",
        "lr_actor = 0.0003\n",
        "#learning rate backward propagation NN state value estimation (Paper recommendation)\n",
        "lr_critic = 0.0003\n",
        "#Number of Learning Iteration we want to perform\n",
        "Iter = 300\n",
        "#Number max of step to realise in one episode.\n",
        "MAX_STEP = 1000\n",
        "#How rewards are discounted. (Paper recommendation)\n",
        "gamma =0.98\n",
        "#How do we stabilize variance in the return computation. (Paper recommendation)\n",
        "lambd = 0.95\n",
        "#batch to train on\n",
        "batch_size = 64\n",
        "# Do we want high change to be taken into account.\n",
        "epsilon = 0.2\n",
        "#weight decay coefficient in ADAM for state value optim.\n",
        "l2_rate = 0.001\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nal_uO4YS-8G"
      },
      "source": [
        "This part is the execution of all the code to train ppo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVT6qclEixR9"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "## TO CHOOSE :\n",
        "#'InvertedPendulum-v4'\n",
        "#'Humanoid-v4'\n",
        "\n",
        "parser.add_argument('--env_name', type=str, default='Humanoid-v4',\n",
        "                    help='name of Mujoco environement')\n",
        "parser.add_argument(\"-f\", required=False)\n",
        "args = parser.parse_args()\n",
        "\n",
        "#We set the environement using gym library completed by Mujoco environnement.\n",
        "#We set the environement to Humanoid-v4 (link: https://www.gymlibrary.dev/environments/mujoco/humanoid/)\n",
        "env = gym.make(args.env_name)\n",
        "\n",
        "\n",
        "#Number of state and action\n",
        "N_S = env.observation_space.shape[0]\n",
        "N_A = env.action_space.shape[0]\n",
        "\n",
        "# Random seed initialization\n",
        "env.seed(500)\n",
        "torch.manual_seed(500)\n",
        "np.random.seed(500)\n",
        "\n",
        "# Run the Ppo class\n",
        "frames = []\n",
        "ppo = Ppo(N_S,N_A)\n",
        "# Normalisation for stability, fast convergence... always good to do.\n",
        "normalize = Normalize(N_S)\n",
        "episodes = 0\n",
        "eva_episodes = 0\n",
        "for iter in range(Iter):\n",
        "    memory = deque()\n",
        "    scores = []\n",
        "    steps = 0\n",
        "    while steps <2048: #Horizon\n",
        "        episodes += 1\n",
        "        s = normalize(env.reset())\n",
        "        score = 0\n",
        "        for _ in range(MAX_STEP):\n",
        "            steps += 1\n",
        "            #Choose an action: detailed in PPO.py\n",
        "            # The action is a numpy array of 17 elements. It means that in the 17 possible directions of action we have a specific value in the continuous space.\n",
        "            # Exemple : the first coordinate correspond to the Torque applied on the hinge in the y-coordinate of the abdomen: this is continuous space.\n",
        "            a=ppo.actor_net.choose_action(torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0))[0]\n",
        "\n",
        "            #Environnement reaction to the action : There is a reaction in the 376 elements that characterize the space :\n",
        "            # Exemple : the first coordinate of the states is the z-coordinate of the torso (centre) and using env.step(a), we get the reaction of this state and\n",
        "            # of all the other ones after the action has been made.\n",
        "            s_ , r ,done,info = env.step(a)\n",
        "            s_ = normalize(s_)\n",
        "\n",
        "            # Do we continue or do we terminate an episode?\n",
        "            mask = (1-done)*1\n",
        "            memory.append([s,a,r,mask])\n",
        "\n",
        "            score += r\n",
        "            s = s_\n",
        "            if done:\n",
        "                break\n",
        "        with open('log_' + args.env_name  + '.txt', 'a') as outfile:\n",
        "            outfile.write('\\t' + str(episodes)  + '\\t' + str(score) + '\\n')\n",
        "        scores.append(score)\n",
        "    score_avg = np.mean(scores)\n",
        "    print('{} episode score is {:.2f}'.format(episodes, score_avg))\n",
        "    ppo.train(memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6BR4M8gTMOD"
      },
      "source": [
        "If we want to continue the train after a first session, we can use the following code.\n",
        "A break has been added in order to stop when we achieve a high score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RijvvhAabKx5"
      },
      "outputs": [],
      "source": [
        "# Code to continue the learning after a break.\n",
        "\n",
        "for iter in range(Iter):\n",
        "    memory = deque()\n",
        "    scores = []\n",
        "    steps = 0\n",
        "    while steps <2048: #Horizon\n",
        "        episodes += 1\n",
        "        s = normalize(env.reset())\n",
        "        score = 0\n",
        "        for _ in range(MAX_STEP):\n",
        "            steps += 1\n",
        "            #Choose an action: detailed in PPO.py\n",
        "            # The action is a numpy array of 17 elements. It means that in the 17 possible directions of action we have a specific value in the continuous space.\n",
        "            # Exemple : the first coordinate correspond to the Torque applied on the hinge in the y-coordinate of the abdomen: this is continuous space.\n",
        "            a=ppo.actor_net.choose_action(torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0))[0]\n",
        "\n",
        "            #Environnement reaction to the action : There is a reaction in the 376 elements that characterize the space :\n",
        "            # Exemple : the first coordinate of the states is the z-coordinate of the torso (centre) and using env.step(a), we get the reaction of this state and\n",
        "            # of all the other ones after the action has been made.\n",
        "            s_ , r ,done,info = env.step(a)\n",
        "            s_ = normalize(s_)\n",
        "\n",
        "            # Do we continue or do we terminate an episode?\n",
        "            mask = (1-done)*1\n",
        "            memory.append([s,a,r,mask])\n",
        "\n",
        "            score += r\n",
        "            s = s_\n",
        "            if done:\n",
        "                break\n",
        "        # We store results of all the episodes in a txt file.\n",
        "        with open('log_' + args.env_name  + '.txt', 'a') as outfile:\n",
        "            outfile.write('\\t' + str(episodes)  + '\\t' + str(score) + '\\n')\n",
        "        scores.append(score)\n",
        "    score_avg = np.mean(scores)\n",
        "    print('{} episode score is {:.2f}'.format(episodes, score_avg))\n",
        "    # Here the magic happens.\n",
        "    ppo.train(memory)\n",
        "    #Add a stop when it is enough\n",
        "    if score_avg>2000:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClSpiGg5Pts8"
      },
      "source": [
        "## Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Vw866HSJ17"
      },
      "source": [
        "### GIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcnIIyMWiryU"
      },
      "outputs": [],
      "source": [
        "# We initialize the environement\n",
        "s = normalize(env.reset())\n",
        "# Initialize visualization\n",
        "Display().start()\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "# List to store frames\n",
        "frames = []\n",
        "# i for me to know how many steps the agent did.\n",
        "\n",
        "for _ in range(500):\n",
        "\n",
        "        # Update the image data\n",
        "      img.set_data(env.render('rgb_array'))\n",
        "\n",
        "      # Capture the current frame\n",
        "      frame = env.render('rgb_array')\n",
        "      frames.append(frame)\n",
        "\n",
        "      # Update the display\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "      #Choose an action according to the environnement s.\n",
        "      a=ppo.actor_net.choose_action(torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0))[0]\n",
        "\n",
        "      #Environnement reaction to the action.\n",
        "      s_ , r ,done,info = env.step(a)\n",
        "      # Update the state in the good format(normalized).\n",
        "      s_ = normalize(s_)\n",
        "      s = s_\n",
        "      # Do we continue or do we terminate an episode?\n",
        "      mask = (1-done)*1\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "# Save frames as a GIF\n",
        "imageio.mimsave('Humanoid_afterv3.gif', frames)\n",
        "\n",
        "# Close the environment and virtual display\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucEefdjkSM6E"
      },
      "source": [
        "### Learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyz8MrYPQebQ"
      },
      "outputs": [],
      "source": [
        "episodes = []\n",
        "scores = []\n",
        "with open('log_' +args.env_name+ '.txt', 'r') as infile:\n",
        "    for line in infile:\n",
        "        data = line.strip().split('\\t')\n",
        "        episodes.append(int(data[0]))\n",
        "        scores.append(float(data[1]))\n",
        "\n",
        "df = pd.DataFrame({'episode': episodes, 'score': scores})\n",
        "grouped = df.groupby('episode')['score']\n",
        "mean_scores = grouped.mean()\n",
        "upper_bounds = grouped.max()\n",
        "lower_bounds = grouped.min()\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Plotting mean scores\n",
        "plt.plot(mean_scores.index, mean_scores.values, label='Score', color='red')\n",
        "# Plotting upper and lower bounds\n",
        "#plt.fill_between(mean_scores.index, lower_bounds.values, upper_bounds.values, color='tomato', alpha=0.3, label='Bounds')\n",
        "plt.title('Score evolution '+' of ' + args.env_name)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig('Humanoid_learning.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}